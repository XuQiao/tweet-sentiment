{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import re, string\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit\n",
    "from transformers import *\n",
    "from featureEng import *\n",
    "import tokenizers\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "print('TF version',tf.__version__)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, LeakyReLU, Dense, Flatten, Activation, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(train, multiple = 1):\n",
    "    def aug(row):\n",
    "        tweet = row['text']\n",
    "        selected_text = row['selected_text']\n",
    "        idx = tweet.find(selected_text) \n",
    "        if idx>=0:\n",
    "            new_tweets = []\n",
    "            prev = tweet[:idx].split()\n",
    "            after = tweet[idx+len(selected_text):].split()\n",
    "            pool = [(i,j) for i in range(len(prev)+1) for j in range(len(after)+1)]\n",
    "            pool.remove((len(prev),0))\n",
    "            if len(pool) == 0:\n",
    "                return None\n",
    "            for r in np.random.choice(len(pool), multiple):\n",
    "                r1, r2 = pool[r]\n",
    "                start = ''\n",
    "                end = ''\n",
    "                if r1 > 0:\n",
    "                    start =' '.join(prev[r1:]) + ' '\n",
    "                if r2 > 0:\n",
    "                    end = ' '+' '.join(after[:r2])\n",
    "                \n",
    "                new_tweets.append(start+selected_text+end)\n",
    "            if len(new_tweets) > 0:\n",
    "                return new_tweets\n",
    "            return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    train_aug = {'text':[],'selected_text':[], 'sentiment':[], 'textID':[]}\n",
    "    for i in train.index:\n",
    "        new_tweets = aug(train.loc[i])\n",
    "        if new_tweets:\n",
    "            for new_tweet in new_tweets:\n",
    "                train_aug['text'].append(new_tweet)\n",
    "                train_aug['selected_text'].append(train.loc[i,'selected_text'])\n",
    "                train_aug['sentiment'].append(train.loc[i,'sentiment'])\n",
    "                train_aug['textID'].append(train.loc[i,'textID'])\n",
    "    train_aug = pd.DataFrame(train_aug).dropna()\n",
    "    return train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_fraction(text):\n",
    "    \"\"\"function to calculate the fraction of unique words on total words of the text\"\"\"\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    unique_count = list(set(text_splited)).__len__()\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return (unique_count/word_count)\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "def stopwords_count(text):\n",
    "    \"\"\" Number of stopwords fraction in a text\"\"\"\n",
    "    text = text.lower()\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    stopwords_count = len([w for w in text_splited if w in eng_stopwords])\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return (stopwords_count/word_count)\n",
    "\n",
    "\n",
    "def punctuations_fraction(text):\n",
    "    \"\"\"functiopn to claculate the fraction of punctuations over total number of characters for a given text \"\"\"\n",
    "    char_count = len(text)\n",
    "    punctuation_count = len([c for c in text if c in string.punctuation])\n",
    "    if char_count == 0:\n",
    "        return 0\n",
    "    return (punctuation_count/char_count)\n",
    "\n",
    "\n",
    "def char_count(text):\n",
    "    \"\"\"function to return number of chracters \"\"\"\n",
    "    return len(text)\n",
    "\n",
    "def fraction_noun(text):\n",
    "    \"\"\"function to give us fraction of noun over total words \"\"\"\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return (noun_count/word_count)\n",
    "\n",
    "def fraction_adj(text):\n",
    "    \"\"\"function to give us fraction of adjectives over total words in given text\"\"\"\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return (adj_count/word_count)\n",
    "\n",
    "def fraction_verbs(text):\n",
    "    \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return (verbs_count/word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "\n",
    "class CustomizedBertForQA:\n",
    "    def __init__(self, tokenizer, config_file, bert_model_file, max_length, n_splits):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentiment_id = {s:self.tokenizer.encode(s).ids[0] for s in ['positive', 'negative', 'neutral']}\n",
    "        self.sentiment_mapping = {'positive':0,'negative':1,'neutral':2}\n",
    "        self.config_file = config_file\n",
    "        self.bert_model_file = bert_model_file\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def bert_data_transform(self, data, train=True):\n",
    "        '''\n",
    "        Transform data into arrays that BERT understands \n",
    "        '''\n",
    "        data = data.reset_index(drop=True)\n",
    "        ct = data.shape[0]\n",
    "        input_ids = np.ones((ct,self.max_length),dtype='int32')\n",
    "        input_ids_nosent = np.ones((ct,self.max_length),dtype='int32')\n",
    "        attention_mask = np.zeros((ct,self.max_length),dtype='int32')\n",
    "        #token_type_ids = np.zeros((ct,self.max_length),dtype='int32')\n",
    "        if train:\n",
    "            start_tokens = np.zeros((ct,self.max_length),dtype='int32')\n",
    "            end_tokens = np.zeros((ct,self.max_length),dtype='int32')\n",
    "            #start_tokens = np.zeros((ct,1),dtype='float32')\n",
    "            #end_tokens = np.zeros((ct,1),dtype='float32')\n",
    "            \n",
    "        for k in range(ct):\n",
    "            # FIND OVERLAP\n",
    "            text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n",
    "            enc = self.tokenizer.encode(text1)\n",
    "            s_tok = self.sentiment_id[data.loc[k,'sentiment']]\n",
    "            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "            input_ids_nosent[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [2] + [2]\n",
    "            attention_mask[k,:len(enc.ids)+5] = 1\n",
    "            \n",
    "            if train:\n",
    "                text2 = \" \".join(data.loc[k,'selected_text'].split())\n",
    "                idx = text1.find(text2)\n",
    "                chars = np.zeros((len(text1)))\n",
    "                chars[idx:idx+len(text2)]=1\n",
    "                if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "                # ID_OFFSETS\n",
    "                offsets = []; idx=0\n",
    "                for t in enc.ids:\n",
    "                    w = self.tokenizer.decode([t])\n",
    "                    offsets.append((idx,idx+len(w)))\n",
    "                    idx += len(w)\n",
    "\n",
    "                # START END TOKENS\n",
    "                toks = []\n",
    "                for i,(a,b) in enumerate(offsets):\n",
    "                    sm = np.sum(chars[a:b])\n",
    "                    if sm>0: toks.append(i) \n",
    "\n",
    "                if len(toks)>0:\n",
    "                    start_tokens[k,toks[0]+1] = 1\n",
    "                    end_tokens[k,toks[-1]+1] = 1\n",
    "                #    start_tokens[k] = (toks[0]+1)/len(enc.ids)\n",
    "                #    end_tokens[k] = (toks[-1]+1)/len(enc.ids)\n",
    "        if train:\n",
    "            return (input_ids, input_ids_nosent, attention_mask, start_tokens, end_tokens)\n",
    "        else:\n",
    "            return (input_ids, input_ids_nosent, attention_mask)\n",
    "    \n",
    "    def customized_features(self, data, input_ids, version, fold):\n",
    "        '''\n",
    "        You can design any type of features here, also need to change code in build_model correspondingly and\n",
    "        change code whereever you use the features\n",
    "        '''\n",
    "        ct = data.shape[0]\n",
    "        add_features = np.zeros((ct,27),dtype='float')\n",
    "        add_features_2D = np.zeros((ct,self.max_length,0),dtype='float')\n",
    "        \n",
    "        sentiment_model = self.build_sentiment_model()\n",
    "        sentiment_model.load_weights('%s-sentiment-%i.h5'%(version,fold))\n",
    "            \n",
    "        layer_output = sentiment_model.get_layer(\"MaxPooling1D\").output\n",
    "        intermediate_model = Model(inputs=sentiment_model.input,outputs=layer_output)\n",
    "        #add_features_2D[:,:,0] = tf.nn.softmax(intermediate_model.predict(input_ids))\n",
    "        #add_features[:,3:6] = sentiment_model.predict(input_ids)\n",
    "        data = featureEng(data,training=False)\n",
    "        data = data.drop(['textID','text','selected_text','sentiment'],axis=1,errors='ignore')\n",
    "        add_features = data.values\n",
    "        return add_features, add_features_2D\n",
    "    \n",
    "    def build_model(self, add_features_shape, add_features_2D_shape):\n",
    "        '''\n",
    "        Add layer on top of BERT\n",
    "        '''        \n",
    "        ids = Input((self.max_length,), dtype=tf.int32)\n",
    "        att = Input((self.max_length,), dtype=tf.int32)\n",
    "        #tok = Input((self.length,), dtype=tf.int32)\n",
    "\n",
    "        add_features = Input((add_features_shape[1],),dtype=tf.float32)\n",
    "        add_features_2D = Input((add_features_2D_shape[1],add_features_2D_shape[2]), dtype=tf.float32)\n",
    "        \n",
    "        config = RobertaConfig.from_pretrained(self.config_file)\n",
    "        bert_model = TFRobertaModel.from_pretrained(self.bert_model_file,config=config)\n",
    "        \n",
    "        x = bert_model(ids,attention_mask=att)\n",
    "        \n",
    "        def output_layer(bert_output, add_features, add_features_2D, name='start'):\n",
    "            x_bert = Dropout(0.3)(bert_output)\n",
    "            x_bert = Conv1D(128, 3,padding='same')(x_bert)\n",
    "            x_bert = LeakyReLU()(x_bert)\n",
    "            x_bert = Conv1D(64, 3,padding='same')(x_bert)\n",
    "            x_bert = Dense(1)(x_bert)\n",
    "            #x_bert = GlobalAveragePooling1D(data_format='channels_first')(x_bert)\n",
    "            x_bert = Reshape((self.max_length, 1))(x_bert)\n",
    "            #x_output = Flatten()(x_bert)\n",
    "            \n",
    "            x_add = Dense(self.max_length, input_dim=add_features_shape[0], activation = 'relu')(add_features)\n",
    "            x_add = Reshape((self.max_length, 1))(x_add)\n",
    "            x_add_2D = add_features_2D\n",
    "            x_combined = Concatenate()([x_bert, x_add, x_add_2D])\n",
    "            x_combined = Conv1D(128, 4, padding='same')(x_combined)\n",
    "            x_combined = Dense(64, activation = 'linear')(x_combined)            \n",
    "            x_combined = Dense(1, activation = 'relu')(x_combined)            \n",
    "            x_combined = Flatten()(x_combined)\n",
    "            #x_output = Dense(1, activation = 'sigmoid')(x_combined)\n",
    "            x_output = Activation('softmax', name=name)(x_combined)\n",
    "             \n",
    "            return x_output\n",
    "        \n",
    "        x1_output = output_layer(x[0], add_features, add_features_2D, name='start')\n",
    "        x2_output = output_layer(x[0], add_features, add_features_2D, name='end')\n",
    "    \n",
    "        model = Model(inputs=[ids, att, add_features, add_features_2D], outputs=[x1_output,x2_output])\n",
    "        optimizer = Adam(learning_rate=3e-5)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['acc'])\n",
    "        #model.compile(loss='mse', optimizer=optimizer, metrics=['acc'])\n",
    "        \n",
    "        # print(model.summary)\n",
    "        # K.clear_session()\n",
    "            \n",
    "        return model\n",
    "        \n",
    "    def train_model(self, train, epochs=3, batch_size=32, version='v0', model_path=None, verbose=1):\n",
    "        # USE verbose=1 FOR INTERACTIVE\n",
    "        jacs = [] \n",
    "\n",
    "        if self.n_splits == 1:\n",
    "            rs = ShuffleSplit(n_splits=1, test_size=0.25, random_state=777)\n",
    "        else:\n",
    "            rs = StratifiedKFold(n_splits=self.n_splits,shuffle=True,random_state=777)\n",
    "        \n",
    "        for fold,(idxT,idxV) in enumerate(rs.split(train.text,train.sentiment.values)):\n",
    "            print('#'*25)\n",
    "            print('### FOLD %i'%(fold+1))\n",
    "            print('#'*25)\n",
    "            train = train.reset_index(drop=True)  \n",
    "            #train = pd.concat([train, data_aug(train.loc[idxT],1)], axis=0, sort=False).reset_index(drop=True)\n",
    "            input_ids, input_ids_nosent, attention_mask, start_tokens, end_tokens = self.bert_data_transform(train, train=True)\n",
    "            oof_start = np.zeros((input_ids.shape[0],self.max_length))\n",
    "            oof_end = np.zeros((input_ids.shape[0],self.max_length))        \n",
    "            #oof_start = np.zeros((input_ids.shape[0],1))\n",
    "            #oof_end = np.zeros((input_ids.shape[0],1))\n",
    "            #idxT = np.append(idxT, range(len(idxT)+len(idxV),len(train)))\n",
    "            K.clear_session()\n",
    "            #print(model.summary)\n",
    "            sentiment_model = self.build_sentiment_model()\n",
    "            \n",
    "            if model_path is None:\n",
    "                #print('Training sentiment model...')\n",
    "                #sv = ModelCheckpoint('%s-sentiment-%i.h5'%(version,fold), monitor='val_loss', \n",
    "                #                     verbose=1,\n",
    "                #                     save_best_only=True,\n",
    "                #                     save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "                #sentiment_model.fit(input_ids_nosent[idxT,], \n",
    "                #                    to_categorical(np.vectorize(self.sentiment_mapping.get)(train.sentiment.values[idxT,])), \n",
    "                #    epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[sv],\n",
    "                #    validation_data=(input_ids_nosent[idxV,], \n",
    "                #                     to_categorical(np.vectorize(self.sentiment_mapping.get)(train.sentiment.values[idxV,])))\n",
    "                #)\n",
    "                add_features, add_features_2D = self.customized_features(train, input_ids, version, fold)\n",
    "                model = self.build_model(add_features.shape, add_features_2D.shape)\n",
    "                print('Training model...')\n",
    "\n",
    "                sv = ModelCheckpoint('%s-roberta-%i.h5'%(version,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                     save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "                es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "                model.fit([input_ids[idxT,], attention_mask[idxT,], add_features[idxT,], add_features_2D[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[sv],\n",
    "                    validation_data=([input_ids[idxV,],attention_mask[idxV,], add_features[idxV,],add_features_2D[idxV,]], \n",
    "                    [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "\n",
    "                #print('Loading model...')\n",
    "                #model.load_weights('%s-roberta-%i.h5'%(version,fold))\n",
    "            else:\n",
    "                add_features, add_features_2D = self.customized_features(train, input_ids, version, fold)\n",
    "                model = self.build_model(add_features.shape, add_features_2D.shape)\n",
    "                print('Loading model...')\n",
    "                model.load_weights('%s/%s-roberta-%i.h5'%(model_path,version,fold))\n",
    "            \n",
    "            for label, idx in {'INF':idxT, 'OOF':idxV}.items():\n",
    "                print('Predicting %s...'%label)\n",
    "                oof_start[idx,],oof_end[idx,] = model.predict([input_ids[idx,],\n",
    "                                                               attention_mask[idx,], \n",
    "                                                               add_features[idx,], \n",
    "                                                               add_features_2D[idx,]],verbose=verbose)\n",
    "\n",
    "                # DISPLAY FOLD JACCARD\n",
    "                all_selected_text = self.predict_decode(train, oof_start, oof_end, idx)\n",
    "                all_jac = []\n",
    "                for i in range(len(idx)):\n",
    "                    #print('text: ', train.loc[idx[i],'text'])\n",
    "                    #print('pred: ',oof_start[idx[i],],oof_end[idx[i],] )\n",
    "                    #print('target: ',start_tokens[idx[i]], end_tokens[idx[i]])\n",
    "                    #print('pred string: ',all_selected_text[i])\n",
    "                    #print('true string: ',train.loc[idx[i],'selected_text'])\n",
    "                    #print(\"------------------------------------\")\n",
    "                    all_jac.append(jaccard(all_selected_text[i],train.loc[idx[i],'selected_text']))\n",
    "                print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all_jac))\n",
    "                \n",
    "                if label == 'OOF':\n",
    "                    jacs.append(np.mean(all_jac))\n",
    "                                 \n",
    "        print('>>>> OVERALL %i Fold CV Jaccard ='%self.n_splits,np.mean(jacs))\n",
    "        \n",
    "        \n",
    "    def predict_test(self, test, version='v0', model_path=None, verbose=1):\n",
    "        test = test.reset_index(drop=True)\n",
    "        input_ids, input_ids_nosent, attention_mask = self.bert_data_transform(test, train=False)\n",
    "        preds_start = np.zeros((input_ids.shape[0],self.max_length))\n",
    "        preds_end = np.zeros((input_ids.shape[0],self.max_length))        \n",
    "        #preds_start = np.zeros((input_ids.shape[0],1))\n",
    "        #preds_end = np.zeros((input_ids.shape[0],1))\n",
    "        \n",
    "        for fold in range(self.n_splits):\n",
    "            K.clear_session()\n",
    "            add_features, add_features_2D = self.customized_features(test, input_ids_nosent, version, fold)\n",
    "            model = self.build_model(add_features.shape, add_features_2D.shape)\n",
    "        \n",
    "            if model_path is None:\n",
    "                model.load_weights('%s-roberta-%i.h5'%(version,fold))\n",
    "            else:\n",
    "                model.load_weights('%s/%s-roberta-%i.h5'%(model_path,version,fold))\n",
    "            print('Predicting Test...')\n",
    "            preds = model.predict([input_ids,attention_mask,add_features,add_features_2D],verbose=verbose)\n",
    "            preds_start += preds[0]/self.n_splits\n",
    "            preds_end += preds[1]/self.n_splits\n",
    "\n",
    "        all_selected_text = self.predict_decode(test, preds_start, preds_end, test.index)\n",
    "\n",
    "        return all_selected_text, test\n",
    "    \n",
    "    def predict_decode(self, text_data, vec_start, vec_end, vec_idx):\n",
    "        all_selected_text = []\n",
    "        for k in vec_idx:\n",
    "            #if text_data.loc[k, 'sentiment'] == 'neutral' or len(text_data.loc[k, 'text'].split())<3: \n",
    "            #    st = text_data.loc[k,'text']\n",
    "            #else:\n",
    "            text1 = \" \"+\" \".join(text_data.loc[k,'text'].split())\n",
    "            enc = self.tokenizer.encode(text1)\n",
    "            a = np.argmax(vec_start[k,])   \n",
    "            b = np.argmax(vec_end[k,])\n",
    "            #a = int(vec_start[k]*len(enc.ids))\n",
    "            #b = int(vec_end[k]*len(enc.ids))\n",
    "            #if a>len(enc.ids) or b<0 or a>b:\n",
    "            if a>b:\n",
    "                st = text_data.loc[k,'text']\n",
    "            else:\n",
    "                st = self.tokenizer.decode(enc.ids[a-1:b])\n",
    "            all_selected_text.append(st)\n",
    "        return pd.Series(all_selected_text)\n",
    "    \n",
    "    def build_sentiment_model(self):\n",
    "        ids = Input((self.max_length,), dtype=tf.int32)\n",
    "        x = Embedding(input_dim=VOCAB_SIZE, input_length=self.max_length, output_dim=512, name=\"Embedding-1\")(ids)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv1D(128, 2,padding='same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Conv1D(self.max_length, 2,padding='same',name='conv1d-2')(x)\n",
    "        x = GlobalMaxPooling1D(name=\"MaxPooling1D\")(x)\n",
    "        x = Dense(3)(x)\n",
    "        out = Activation('softmax',name='softmax')(x)\n",
    "        sentiment_model = Model(inputs=[ids], outputs=[out])\n",
    "        optimizer = Adam(learning_rate=8e-5)\n",
    "        sentiment_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"acc\"])\n",
    "        \n",
    "        return sentiment_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'/vocab-roberta-base.json', \n",
    "    merges_file=PATH+'/merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = 50156\n",
    "SAMPLE_RUN = False # Set True if you just want to debug implementation. Otherwise False.\n",
    "MODEL_PATH = None # Set None if you want to train a new model, otherwise specify the PATH of trained model\n",
    "N_SPLITS = 1 # 1 means train validation split at portion 0.75:0.25\n",
    "MAX_LEN = 108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiao/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "645/645 [==============================] - ETA: 0s - loss: 2.3941 - start_loss: 1.1870 - end_loss: 1.2071 - start_acc: 0.6290 - end_acc: 0.6527\n",
      "Epoch 00001: val_loss improved from inf to 1.73523, saving model to v0-roberta-0.h5\n",
      "645/645 [==============================] - 239s 371ms/step - loss: 2.3941 - start_loss: 1.1870 - end_loss: 1.2071 - start_acc: 0.6290 - end_acc: 0.6527 - val_loss: 1.7352 - val_start_loss: 0.8775 - val_end_loss: 0.8577 - val_start_acc: 0.7030 - val_end_acc: 0.7118\n",
      "Epoch 2/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 1.6551 - start_loss: 0.8511 - end_loss: 0.8040 - start_acc: 0.6995 - end_acc: 0.7204\n",
      "Epoch 00002: val_loss improved from 1.73523 to 1.68600, saving model to v0-roberta-0.h5\n",
      "645/645 [==============================] - 210s 326ms/step - loss: 1.6551 - start_loss: 0.8511 - end_loss: 0.8040 - start_acc: 0.6995 - end_acc: 0.7204 - val_loss: 1.6860 - val_start_loss: 0.8588 - val_end_loss: 0.8272 - val_start_acc: 0.6992 - val_end_acc: 0.7286\n",
      "Epoch 3/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 1.5001 - start_loss: 0.7764 - end_loss: 0.7237 - start_acc: 0.7182 - end_acc: 0.7421\n",
      "Epoch 00003: val_loss improved from 1.68600 to 1.67550, saving model to v0-roberta-0.h5\n",
      "645/645 [==============================] - 211s 326ms/step - loss: 1.5001 - start_loss: 0.7764 - end_loss: 0.7237 - start_acc: 0.7182 - end_acc: 0.7421 - val_loss: 1.6755 - val_start_loss: 0.8655 - val_end_loss: 0.8100 - val_start_acc: 0.6961 - val_end_acc: 0.7246\n",
      "Epoch 4/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 1.3495 - start_loss: 0.7003 - end_loss: 0.6492 - start_acc: 0.7383 - end_acc: 0.7654\n",
      "Epoch 00004: val_loss did not improve from 1.67550\n",
      "645/645 [==============================] - 210s 325ms/step - loss: 1.3495 - start_loss: 0.7003 - end_loss: 0.6492 - start_acc: 0.7383 - end_acc: 0.7654 - val_loss: 1.7433 - val_start_loss: 0.8622 - val_end_loss: 0.8812 - val_start_acc: 0.6945 - val_end_acc: 0.7197\n",
      "Epoch 5/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 1.2121 - start_loss: 0.6317 - end_loss: 0.5804 - start_acc: 0.7594 - end_acc: 0.7898\n",
      "Epoch 00005: val_loss did not improve from 1.67550\n",
      "645/645 [==============================] - 210s 326ms/step - loss: 1.2121 - start_loss: 0.6317 - end_loss: 0.5804 - start_acc: 0.7594 - end_acc: 0.7898 - val_loss: 1.9031 - val_start_loss: 0.9539 - val_end_loss: 0.9492 - val_start_acc: 0.6933 - val_end_acc: 0.7072\n",
      "Predicting INF...\n",
      "645/645 [==============================] - 70s 109ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.8260213964936318\n",
      "Predicting OOF...\n",
      "215/215 [==============================] - 23s 109ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.6995461784276381\n",
      ">>>> OVERALL 1 Fold CV Jaccard = 0.6995461784276381\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['selected_text'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-83a3b8316436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mall_selected_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_selected_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5e5473856114>\u001b[0m in \u001b[0;36mpredict_test\u001b[0;34m(self, test, version, model_path, verbose)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0madd_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_features_2D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomized_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_nosent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_features_2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5e5473856114>\u001b[0m in \u001b[0;36mcustomized_features\u001b[0;34m(self, data, input_ids, version, fold)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#add_features[:,3:6] = sentiment_model.predict(input_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureEng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0madd_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madd_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_features_2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m         )\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['selected_text'] not found in axis\""
     ]
    }
   ],
   "source": [
    "if SAMPLE_RUN:\n",
    "    sample_train = train.sample(100)\n",
    "    sample_test = test.sample(3)\n",
    "\n",
    "    sample_model = CustomizedBertForQA(tokenizer, config_file=PATH+'config-roberta-base.json', \n",
    "                                       bert_model_file=PATH+'pretrained-roberta-base.h5', \n",
    "                                       max_length=MAX_LEN, n_splits=N_SPLITS)\n",
    "    sample_model.train_model(sample_train, epochs=3, batch_size=32, version='v0', model_path=MODEL_PATH, verbose=1)\n",
    "    \n",
    "    sample_selected_text, sample_test = sample_model.predict_test(sample_test, version='v0', model_path=MODEL_PATH, verbose=1)\n",
    "    sample_test['selected_text'] = sample_selected_text\n",
    "    pd.set_option('max_colwidth', 60)\n",
    "    print(sample_test)\n",
    "else:\n",
    "    full_model = CustomizedBertForQA(tokenizer, config_file=PATH+'config-roberta-base.json',\n",
    "                                     bert_model_file=PATH+'pretrained-roberta-base.h5', \n",
    "                                     max_length=MAX_LEN, n_splits=N_SPLITS)\n",
    "    full_model.train_model(train, epochs=5, batch_size=32, version='v0', model_path=MODEL_PATH, verbose=1)\n",
    "\n",
    "    all_selected_text, test = full_model.predict_test(test, version='v0', model_path=MODEL_PATH, verbose=1)\n",
    "    test['selected_text'] = all_selected_text\n",
    "    test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "    pd.set_option('max_colwidth', 60)\n",
    "    print(test.sample(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
