{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=96):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file='../input/roberta-base/vocab.json', \n",
    "            merges_file='../input/roberta-base/merges.txt', \n",
    "            lowercase=True,\n",
    "            add_prefix_space=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        ids, masks, tweet, offsets = self.get_input_data(row)\n",
    "        data['ids'] = ids\n",
    "        data['masks'] = masks\n",
    "        data['tweet'] = tweet\n",
    "        data['offsets'] = offsets\n",
    "        \n",
    "        if self.labeled:\n",
    "            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
    "            data['start_idx'] = start_idx\n",
    "            data['end_idx'] = end_idx\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_input_data(self, row):\n",
    "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "        encoding = self.tokenizer.encode(tweet)\n",
    "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
    "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "                \n",
    "        pad_len = self.max_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids += [1] * pad_len\n",
    "            offsets += [(0, 0)] * pad_len\n",
    "        \n",
    "        ids = torch.tensor(ids)\n",
    "        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "        offsets = torch.tensor(offsets)\n",
    "        \n",
    "        return ids, masks, tweet, offsets\n",
    "        \n",
    "    def get_target_idx(self, row, tweet, offsets):\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "        \n",
    "        return start_idx, end_idx\n",
    "        \n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetModel, self).__init__()\n",
    "        \n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            '../input/roberta-base/config.json', output_hidden_states=True)    \n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            '../input/roberta-base/pytorch_model.bin', config=config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "         \n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "        \n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "    \n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_jaccard = 0.0\n",
    "            \n",
    "            for data in (dataloaders_dict[phase]):\n",
    "                ids = data['ids'].cuda()\n",
    "                masks = data['masks'].cuda()\n",
    "                tweet = data['tweet']\n",
    "                offsets = data['offsets'].numpy()\n",
    "                start_idx = data['start_idx'].cuda()\n",
    "                end_idx = data['end_idx'].cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * len(ids)\n",
    "                    \n",
    "                    start_idx = start_idx.cpu().detach().numpy()\n",
    "                    end_idx = end_idx.cpu().detach().numpy()\n",
    "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "                    \n",
    "                    for i in range(len(ids)):                        \n",
    "                        jaccard_score = compute_jaccard_score(\n",
    "                            tweet[i],\n",
    "                            start_idx[i],\n",
    "                            end_idx[i],\n",
    "                            start_logits[i], \n",
    "                            end_logits[i], \n",
    "                            offsets[i])\n",
    "                        epoch_jaccard += jaccard_score\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
    "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
    "    \n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFold: 1\\nEpoch 1/3 | train | Loss: 2.1345 | Jaccard: 0.6625\\nEpoch 1/3 |  val  | Loss: 1.6840 | Jaccard: 0.7111\\nEpoch 2/3 | train | Loss: 1.5973 | Jaccard: 0.7197\\nEpoch 2/3 |  val  | Loss: 1.6390 | Jaccard: 0.7089\\nEpoch 3/3 | train | Loss: 1.4487 | Jaccard: 0.7375\\nEpoch 3/3 |  val  | Loss: 1.7325 | Jaccard: 0.7080\\nFold: 2\\nEpoch 1/3 | train | Loss: 2.1253 | Jaccard: 0.6630\\nEpoch 1/3 |  val  | Loss: 1.7174 | Jaccard: 0.7069\\nEpoch 2/3 | train | Loss: 1.6150 | Jaccard: 0.7175\\nEpoch 2/3 |  val  | Loss: 1.6510 | Jaccard: 0.7115\\nEpoch 3/3 | train | Loss: 1.4671 | Jaccard: 0.7358\\nEpoch 3/3 |  val  | Loss: 1.6483 | Jaccard: 0.7168\\nFold: 3\\nEpoch 1/3 | train | Loss: 2.1514 | Jaccard: 0.6622\\nEpoch 1/3 |  val  | Loss: 1.6977 | Jaccard: 0.7115\\nEpoch 2/3 | train | Loss: 1.6288 | Jaccard: 0.7153\\nEpoch 2/3 |  val  | Loss: 1.6592 | Jaccard: 0.7108\\nEpoch 3/3 | train | Loss: 1.5064 | Jaccard: 0.7324\\nEpoch 3/3 |  val  | Loss: 1.6697 | Jaccard: 0.7173\\nFold: 4\\nEpoch 1/3 | train | Loss: 2.1097 | Jaccard: 0.6655\\nEpoch 1/3 |  val  | Loss: 1.5261 | Jaccard: 0.7257\\nEpoch 2/3 | train | Loss: 1.5937 | Jaccard: 0.7170\\nEpoch 2/3 |  val  | Loss: 1.5632 | Jaccard: 0.7263\\nEpoch 3/3 | train | Loss: 1.4437 | Jaccard: 0.7394\\nEpoch 3/3 |  val  | Loss: 1.5678 | Jaccard: 0.7263\\nFold: 5\\nEpoch 1/3 | train | Loss: 2.0518 | Jaccard: 0.6724\\nEpoch 1/3 |  val  | Loss: 1.6016 | Jaccard: 0.7077\\nEpoch 2/3 | train | Loss: 1.5821 | Jaccard: 0.7193\\nEpoch 2/3 |  val  | Loss: 1.6334 | Jaccard: 0.7177\\nEpoch 3/3 | train | Loss: 1.4654 | Jaccard: 0.7340\\nEpoch 3/3 |  val  | Loss: 1.6015 | Jaccard: 0.7155\\nFold: 6\\nEpoch 1/3 | train | Loss: 2.0839 | Jaccard: 0.6718\\nEpoch 1/3 |  val  | Loss: 1.6847 | Jaccard: 0.7021\\nEpoch 2/3 | train | Loss: 1.5995 | Jaccard: 0.7184\\nEpoch 2/3 |  val  | Loss: 1.6905 | Jaccard: 0.7131\\nEpoch 3/3 | train | Loss: 1.4767 | Jaccard: 0.7346\\nEpoch 3/3 |  val  | Loss: 1.6158 | Jaccard: 0.7170\\nFold: 7\\nEpoch 1/3 | train | Loss: 2.1359 | Jaccard: 0.6647\\nEpoch 1/3 |  val  | Loss: 1.5968 | Jaccard: 0.7157\\nEpoch 2/3 | train | Loss: 1.6044 | Jaccard: 0.7193\\nEpoch 2/3 |  val  | Loss: 1.5547 | Jaccard: 0.7264\\nEpoch 3/3 | train | Loss: 1.4607 | Jaccard: 0.7333\\nEpoch 3/3 |  val  | Loss: 1.5937 | Jaccard: 0.7212\\nFold: 8\\nEpoch 1/3 | train | Loss: 2.1553 | Jaccard: 0.6641\\nEpoch 1/3 |  val  | Loss: 1.6965 | Jaccard: 0.7080\\nEpoch 2/3 | train | Loss: 1.5869 | Jaccard: 0.7188\\nEpoch 2/3 |  val  | Loss: 1.5842 | Jaccard: 0.7213\\nEpoch 3/3 | train | Loss: 1.4387 | Jaccard: 0.7387\\nEpoch 3/3 |  val  | Loss: 1.6391 | Jaccard: 0.7186\\nFold: 9\\nEpoch 1/3 | train | Loss: 2.1891 | Jaccard: 0.6554\\nEpoch 1/3 |  val  | Loss: 1.7496 | Jaccard: 0.7026\\nEpoch 2/3 | train | Loss: 1.6390 | Jaccard: 0.7147\\nEpoch 2/3 |  val  | Loss: 1.6708 | Jaccard: 0.7145\\nEpoch 3/3 | train | Loss: 1.5086 | Jaccard: 0.7284\\nEpoch 3/3 |  val  | Loss: 1.6404 | Jaccard: 0.7149\\nFold: 10\\nEpoch 1/3 | train | Loss: 2.1928 | Jaccard: 0.6548\\nEpoch 1/3 |  val  | Loss: 1.6318 | Jaccard: 0.7099\\nEpoch 2/3 | train | Loss: 1.6129 | Jaccard: 0.7170\\nEpoch 2/3 |  val  | Loss: 1.5790 | Jaccard: 0.7224\\nEpoch 3/3 | train | Loss: 1.4701 | Jaccard: 0.7343\\nEpoch 3/3 |  val  | Loss: 1.6129 | Jaccard: 0.7157\\nCPU times: user 1h 17min 48s, sys: 3min 31s, total: 1h 21min 20s\\nWall time: 1h 21min 32s\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "#train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "#train_df['text'] = train_df['text'].astype(str)\n",
    "#train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
    "\n",
    "#for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "#    print(f'Fold: {fold}')\n",
    "\n",
    "#    model = TweetModel()\n",
    "#    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "#    criterion = loss_fn    \n",
    "#    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "#    train_model(\n",
    "#        model, \n",
    "#        dataloaders_dict,\n",
    "#        criterion, \n",
    "#        optimizer, \n",
    "#        num_epochs,\n",
    "#        f'roberta_fold{fold}.pth')\n",
    "'''\n",
    "Fold: 1\n",
    "Epoch 1/3 | train | Loss: 2.1345 | Jaccard: 0.6625\n",
    "Epoch 1/3 |  val  | Loss: 1.6840 | Jaccard: 0.7111\n",
    "Epoch 2/3 | train | Loss: 1.5973 | Jaccard: 0.7197\n",
    "Epoch 2/3 |  val  | Loss: 1.6390 | Jaccard: 0.7089\n",
    "Epoch 3/3 | train | Loss: 1.4487 | Jaccard: 0.7375\n",
    "Epoch 3/3 |  val  | Loss: 1.7325 | Jaccard: 0.7080\n",
    "Fold: 2\n",
    "Epoch 1/3 | train | Loss: 2.1253 | Jaccard: 0.6630\n",
    "Epoch 1/3 |  val  | Loss: 1.7174 | Jaccard: 0.7069\n",
    "Epoch 2/3 | train | Loss: 1.6150 | Jaccard: 0.7175\n",
    "Epoch 2/3 |  val  | Loss: 1.6510 | Jaccard: 0.7115\n",
    "Epoch 3/3 | train | Loss: 1.4671 | Jaccard: 0.7358\n",
    "Epoch 3/3 |  val  | Loss: 1.6483 | Jaccard: 0.7168\n",
    "Fold: 3\n",
    "Epoch 1/3 | train | Loss: 2.1514 | Jaccard: 0.6622\n",
    "Epoch 1/3 |  val  | Loss: 1.6977 | Jaccard: 0.7115\n",
    "Epoch 2/3 | train | Loss: 1.6288 | Jaccard: 0.7153\n",
    "Epoch 2/3 |  val  | Loss: 1.6592 | Jaccard: 0.7108\n",
    "Epoch 3/3 | train | Loss: 1.5064 | Jaccard: 0.7324\n",
    "Epoch 3/3 |  val  | Loss: 1.6697 | Jaccard: 0.7173\n",
    "Fold: 4\n",
    "Epoch 1/3 | train | Loss: 2.1097 | Jaccard: 0.6655\n",
    "Epoch 1/3 |  val  | Loss: 1.5261 | Jaccard: 0.7257\n",
    "Epoch 2/3 | train | Loss: 1.5937 | Jaccard: 0.7170\n",
    "Epoch 2/3 |  val  | Loss: 1.5632 | Jaccard: 0.7263\n",
    "Epoch 3/3 | train | Loss: 1.4437 | Jaccard: 0.7394\n",
    "Epoch 3/3 |  val  | Loss: 1.5678 | Jaccard: 0.7263\n",
    "Fold: 5\n",
    "Epoch 1/3 | train | Loss: 2.0518 | Jaccard: 0.6724\n",
    "Epoch 1/3 |  val  | Loss: 1.6016 | Jaccard: 0.7077\n",
    "Epoch 2/3 | train | Loss: 1.5821 | Jaccard: 0.7193\n",
    "Epoch 2/3 |  val  | Loss: 1.6334 | Jaccard: 0.7177\n",
    "Epoch 3/3 | train | Loss: 1.4654 | Jaccard: 0.7340\n",
    "Epoch 3/3 |  val  | Loss: 1.6015 | Jaccard: 0.7155\n",
    "Fold: 6\n",
    "Epoch 1/3 | train | Loss: 2.0839 | Jaccard: 0.6718\n",
    "Epoch 1/3 |  val  | Loss: 1.6847 | Jaccard: 0.7021\n",
    "Epoch 2/3 | train | Loss: 1.5995 | Jaccard: 0.7184\n",
    "Epoch 2/3 |  val  | Loss: 1.6905 | Jaccard: 0.7131\n",
    "Epoch 3/3 | train | Loss: 1.4767 | Jaccard: 0.7346\n",
    "Epoch 3/3 |  val  | Loss: 1.6158 | Jaccard: 0.7170\n",
    "Fold: 7\n",
    "Epoch 1/3 | train | Loss: 2.1359 | Jaccard: 0.6647\n",
    "Epoch 1/3 |  val  | Loss: 1.5968 | Jaccard: 0.7157\n",
    "Epoch 2/3 | train | Loss: 1.6044 | Jaccard: 0.7193\n",
    "Epoch 2/3 |  val  | Loss: 1.5547 | Jaccard: 0.7264\n",
    "Epoch 3/3 | train | Loss: 1.4607 | Jaccard: 0.7333\n",
    "Epoch 3/3 |  val  | Loss: 1.5937 | Jaccard: 0.7212\n",
    "Fold: 8\n",
    "Epoch 1/3 | train | Loss: 2.1553 | Jaccard: 0.6641\n",
    "Epoch 1/3 |  val  | Loss: 1.6965 | Jaccard: 0.7080\n",
    "Epoch 2/3 | train | Loss: 1.5869 | Jaccard: 0.7188\n",
    "Epoch 2/3 |  val  | Loss: 1.5842 | Jaccard: 0.7213\n",
    "Epoch 3/3 | train | Loss: 1.4387 | Jaccard: 0.7387\n",
    "Epoch 3/3 |  val  | Loss: 1.6391 | Jaccard: 0.7186\n",
    "Fold: 9\n",
    "Epoch 1/3 | train | Loss: 2.1891 | Jaccard: 0.6554\n",
    "Epoch 1/3 |  val  | Loss: 1.7496 | Jaccard: 0.7026\n",
    "Epoch 2/3 | train | Loss: 1.6390 | Jaccard: 0.7147\n",
    "Epoch 2/3 |  val  | Loss: 1.6708 | Jaccard: 0.7145\n",
    "Epoch 3/3 | train | Loss: 1.5086 | Jaccard: 0.7284\n",
    "Epoch 3/3 |  val  | Loss: 1.6404 | Jaccard: 0.7149\n",
    "Fold: 10\n",
    "Epoch 1/3 | train | Loss: 2.1928 | Jaccard: 0.6548\n",
    "Epoch 1/3 |  val  | Loss: 1.6318 | Jaccard: 0.7099\n",
    "Epoch 2/3 | train | Loss: 1.6129 | Jaccard: 0.7170\n",
    "Epoch 2/3 |  val  | Loss: 1.5790 | Jaccard: 0.7224\n",
    "Epoch 3/3 | train | Loss: 1.4701 | Jaccard: 0.7343\n",
    "Epoch 3/3 |  val  | Loss: 1.6129 | Jaccard: 0.7157\n",
    "CPU times: user 1h 17min 48s, sys: 3min 31s, total: 1h 21min 20s\n",
    "Wall time: 1h 21min 32s\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 4.12 s, total: 1min 44s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "test_loader = get_test_loader(test_df)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = TweetModel()\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "for data in test_loader:\n",
    "    ids = data['ids'].cuda()\n",
    "    masks = data['masks'].cuda()\n",
    "    tweet = data['tweet']\n",
    "    offsets = data['offsets'].numpy()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):    \n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        if start_pred > end_pred:\n",
    "            pred = tweet[i]\n",
    "        else:\n",
    "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>such a shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>i like it!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID             selected_text\n",
       "0  f87dea47db   last session of the day\n",
       "1  96d74cb729                  exciting\n",
       "2  eee518ae67             such a shame!\n",
       "3  01082688c6               happy bday!\n",
       "4  33987a8ee5               i like it!!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "sub_df['selected_text'] = predictions\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=96\n",
    "def get_input_data(row,tokenizer,max_len):\n",
    "    tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "    encoding = tokenizer.encode(tweet)\n",
    "    sentiment_id = tokenizer.encode(row.sentiment).ids\n",
    "    ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "\n",
    "    pad_len = max_len - len(ids)\n",
    "    if pad_len > 0:\n",
    "        ids += [1] * pad_len\n",
    "        offsets += [(0, 0)] * pad_len\n",
    "\n",
    "    ids = torch.tensor(ids)\n",
    "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "    offsets = torch.tensor(offsets)\n",
    "\n",
    "    return ids, masks, tweet, offsets\n",
    "\n",
    "def get_target_idx(row, tweet, offsets):\n",
    "    selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    start_idx = target_idx[0]\n",
    "    end_idx = target_idx[-1]\n",
    "\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def check(train):\n",
    "    jacs =[]\n",
    "    for x in train.index:\n",
    "        row=train.loc[x]\n",
    "        d=get_input_data(row,tokenizer,max_len)\n",
    "        #print(d[0],d[1],d[2],d[3])\n",
    "        start_idx, end_idx = get_target_idx(row, d[2], d[3])\n",
    "        #print(start_idx, end_idx)\n",
    "        pred_text = get_selected_text(row['text'], start_idx, end_idx, d[3].numpy())\n",
    "        #print(pred_text)\n",
    "        decoded = jaccard(pred_text,row['selected_text'])\n",
    "        jacs.append(decoded)\n",
    "    return np.mean(jacs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9155208251804862"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
    "check(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
